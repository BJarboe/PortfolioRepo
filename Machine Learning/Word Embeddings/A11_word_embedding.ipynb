{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning - Irfan Khan \n",
    "# Assignment 11: Word Embeddings\n",
    "\n",
    "**Total points: 15**\n",
    "\n",
    "Updated assignment designed by Ex Prof Yang Xu, Computer Science department, SDSU. In this assignment, you will implement a simple continuous bag-of-words (CBOW) model that uses surrounding context words to predict the target word in the middle. https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html?highlight=embeddings\n",
    "\n",
    "You will also use pre-trained GloVe embeddings from https://nlp.stanford.edu/projects/glove/ to predict target word in the middle. \n",
    "\n",
    "To index into the embeddings, you must use torch.LongTensor (since the indices are integers, not floats)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_vocab function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(words: List[str]):\n",
    "    vocab = Counter()\n",
    "    for w in words:\n",
    "        vocab[w] += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Prepare data\n",
    "**2 points**\n",
    "\n",
    "In this task, you should prepare your data, which is a list of tuples. Each tuple has two elements: a list of context words, and the target word.\n",
    "Here both context words and target word are represented by word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.', 'Computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers.', 'As', 'they', 'evolve,', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data.', 'The', 'evolution', 'of', 'a', 'process', 'is', 'directed', 'by', 'a', 'pattern', 'of', 'rules', 'called', 'a', 'program.', 'People', 'create', 'programs', 'to', 'direct', 'processes.', 'In', 'effect,', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells.']\n",
      "Counter({'of': 4, 'a': 4, 'the': 3, 'are': 2, 'to': 2, 'processes': 2, 'abstract': 2, 'called': 2, 'We': 1, 'about': 1, 'study': 1, 'idea': 1, 'computational': 1, 'process.': 1, 'Computational': 1, 'beings': 1, 'that': 1, 'inhabit': 1, 'computers.': 1, 'As': 1, 'they': 1, 'evolve,': 1, 'manipulate': 1, 'other': 1, 'things': 1, 'data.': 1, 'The': 1, 'evolution': 1, 'process': 1, 'is': 1, 'directed': 1, 'by': 1, 'pattern': 1, 'rules': 1, 'program.': 1, 'People': 1, 'create': 1, 'programs': 1, 'direct': 1, 'processes.': 1, 'In': 1, 'effect,': 1, 'we': 1, 'conjure': 1, 'spirits': 1, 'computer': 1, 'with': 1, 'our': 1, 'spells.': 1})\n",
      "vocab_size 49\n",
      "word_to_idx {'We': 0, 'are': 1, 'about': 2, 'to': 3, 'study': 4, 'the': 5, 'idea': 6, 'of': 7, 'a': 8, 'computational': 9, 'process.': 10, 'Computational': 11, 'processes': 12, 'abstract': 13, 'beings': 14, 'that': 15, 'inhabit': 16, 'computers.': 17, 'As': 18, 'they': 19, 'evolve,': 20, 'manipulate': 21, 'other': 22, 'things': 23, 'called': 24, 'data.': 25, 'The': 26, 'evolution': 27, 'process': 28, 'is': 29, 'directed': 30, 'by': 31, 'pattern': 32, 'rules': 33, 'program.': 34, 'People': 35, 'create': 36, 'programs': 37, 'direct': 38, 'processes.': 39, 'In': 40, 'effect,': 41, 'we': 42, 'conjure': 43, 'spirits': 44, 'computer': 45, 'with': 46, 'our': 47, 'spells.': 48}\n",
      "idx_to_word ['We', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process.', 'Computational', 'processes', 'abstract', 'beings', 'that', 'inhabit', 'computers.', 'As', 'they', 'evolve,', 'manipulate', 'other', 'things', 'called', 'data.', 'The', 'evolution', 'process', 'is', 'directed', 'by', 'pattern', 'rules', 'program.', 'People', 'create', 'programs', 'direct', 'processes.', 'In', 'effect,', 'we', 'conjure', 'spirits', 'computer', 'with', 'our', 'spells.']\n",
      "word_indices [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 13, 14, 15, 16, 17, 18, 19, 20, 12, 21, 22, 13, 23, 24, 25, 26, 27, 7, 8, 28, 29, 30, 31, 8, 32, 7, 33, 24, 8, 34, 35, 36, 37, 3, 38, 39, 40, 41, 42, 43, 5, 44, 7, 5, 45, 46, 47, 48]\n",
      "len(word_indices) 62\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 3  # Define the context size. Default value 3, which means the context \n",
    "#includes 3 words to the left, 3 to the right\n",
    "\n",
    "#In Python, enclosing a string within triple quotes (either single ' or double \" quotes)\n",
    "#allows you to create a multiline string literal\n",
    "\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "print (raw_text) #- try it out\n",
    "vocab = build_vocab(raw_text)\n",
    "print (vocab) #- try it out\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print ('vocab_size',vocab_size)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}#index for each word in vocab\n",
    "idx_to_word = list(vocab)#list of words in the vocab\n",
    "\n",
    "print ('word_to_idx', word_to_idx) #- try it out\n",
    "print ('idx_to_word',idx_to_word) #- try it out\n",
    "\n",
    "word_indices = [word_to_idx[w] for w in raw_text]#Every word in the raw text has an index\n",
    "\n",
    "print ('word_indices',word_indices)\n",
    "print ('len(word_indices)', len(word_indices))\n",
    "\n",
    "#The function prepare_data creates a data sample with 6 context words and the target word between them\n",
    "def prepare_data(word_indices):\n",
    "    data = []\n",
    "    for i in range(CONTEXT_SIZE, len(word_indices) - CONTEXT_SIZE):\n",
    "\n",
    "        #### START YOUR CODE ####\n",
    "        # Hint: You can intialize context to an empty list\n",
    "        # and then use a for loop to append elements to context properly.\n",
    "\n",
    "        \"\"\"\n",
    "        [..CONTEXT] target [CONTEXT..]\n",
    "        \"\"\"\n",
    "\n",
    "        target = word_indices[i]\n",
    "\n",
    "        context = []\n",
    "        for j in range(i - CONTEXT_SIZE, i + CONTEXT_SIZE + 1):\n",
    "            if j != i:\n",
    "                context.append(word_indices[j])\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "        data.append((context, target))\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "vocab_size 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "49\n",
      "<class 'list'>\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))\n",
    "print(len(vocab))\n",
    "print(type(word_indices))\n",
    "print(len(word_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "<class 'collections.Counter'><br>\n",
    "49<br>\n",
    "<class 'list'><br>\n",
    "62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data 56\n",
      "data[0]: ([0, 1, 2, 4, 5, 6], 3)\n",
      "context words: ['We', 'are', 'about', 'study', 'the', 'idea']\n",
      "target word: to\n"
     ]
    }
   ],
   "source": [
    "# Test Task 1. Do not change the code below.\n",
    "data = prepare_data(word_indices)\n",
    "print ('length of data', len(data))\n",
    "print('data[0]:', data[0])\n",
    "ctx, tgt = data[0]\n",
    "print('context words:', [idx_to_word[c] for c in ctx])\n",
    "print('target word:', idx_to_word[tgt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected output\n",
    "\n",
    "length of data 56<br>\n",
    "data[0]: ([0, 1, 2, 4, 5, 6], 3)<br>\n",
    "context words: ['We', 'are', 'about', 'study', 'the', 'idea']<br>\n",
    "target word: to\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement a CBOW model\n",
    "\n",
    "**5 points**\n",
    "\n",
    "In this task, you will implement a CBOW model. In the `__init__()` method, define the size of `self.embeddings` and `self.linear` properly.\n",
    "\n",
    "The `self.linear` takes the sum of embeddings of all context words as input, and the output size is `vocab_size`.\n",
    "It is followed by a softmax activation (`nn.LogSoftmax`).\n",
    "\n",
    "The `forward()` method has a input argument `inputs`, which is the context word indices (in a `torch.long` tensor).\n",
    "You should get the embeddings of all context words, and compute the sum (into the `embeds` variable). \n",
    "\n",
    "Make this class definition flexible to receive embed_weights from a pre-trained model which will be needed for section 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, no_of_samples, embed_weights):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #### START YOUR CODE ####\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        #Hint use nn.Embedding() if no initial pre-trained embeddings in embed_weights are provided\n",
    "        if embed_weights is not None:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embed_weights, freeze=False)\n",
    "        #Use nn.Embedding.from_pretrained() if embed_weights are provided.\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        #### END YOUR CODE ####\n",
    "        \n",
    "        self.act = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        #### START YOUR CODE ####\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        #### END YOUR CODE ####\n",
    "        \n",
    "        out = self.linear(embeds)\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_output.shape torch.Size([1, 10])\n",
      "test_output tensor([[-1.6878, -4.2108, -5.0252, -2.9802, -3.1362, -1.5436, -1.4120, -3.2485,\n",
      "         -1.6490, -4.5009]])\n"
     ]
    }
   ],
   "source": [
    "# Test Task 2. Do not change the code blow\n",
    "torch.manual_seed(0)\n",
    "\n",
    "m = CBOW(10, 20, 3, embed_weights=None)\n",
    "test_input = torch.tensor([1,2,3], dtype=torch.long)\n",
    "\n",
    "test_output = m(test_input)\n",
    "\n",
    "print('test_output.shape', test_output.shape)\n",
    "print('test_output', test_output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "test_output.shape torch.Size([1, 10])<br>\n",
    "test_output tensor([[-1.6878, -4.2108, -5.0252, -2.9802, -3.1362, -1.5436, -1.4120, -3.2485,\n",
    "         -1.6490, -4.5009]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Training loop\n",
    "**2 points**\n",
    "\n",
    "In this task, you will complete the training loop. \n",
    "\n",
    "You should create `ctx_tensor` and `tgt_tensor` out of `ctx` and `tgt`, respectively. *Hint*: you need to put `tgt` to a list before creating the `tgt_tensor`, so that the resulting tensor is of the correct dimension that is acceptable to `nn.NLLLoss()`.\n",
    "\n",
    "`ctx_tensor` is used to compute `output`. `loss_function()` is called upon `output` and `tgt_tensor` to compute the loss. total_loss contains the total loss at each epoch.\n",
    "\n",
    "We are using an embedding dimension of 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss within epoch 5:  182.17068481445312\n",
      "Loss within epoch 10:  128.0272674560547\n",
      "Loss within epoch 15:  93.20354461669922\n",
      "Loss within epoch 20:  69.58182525634766\n",
      "Loss within epoch 25:  53.30769729614258\n",
      "Loss within epoch 30:  41.98340606689453\n",
      "Loss within epoch 35:  33.98306655883789\n",
      "Loss within epoch 40:  28.213909149169922\n",
      "Loss within epoch 45:  23.947216033935547\n",
      "Loss within epoch 50:  20.706602096557617\n",
      "Loss within epoch 55:  18.183002471923828\n",
      "Loss within epoch 60:  16.173582077026367\n",
      "Loss within epoch 65:  14.542236328125\n",
      "Loss within epoch 70:  13.195314407348633\n",
      "Loss within epoch 75:  12.066819190979004\n",
      "Loss within epoch 80:  11.109158515930176\n",
      "Loss within epoch 85:  10.287293434143066\n",
      "Loss within epoch 90:  9.574957847595215\n",
      "Loss within epoch 95:  8.952116012573242\n",
      "Loss within epoch 100:  8.403237342834473\n",
      "Loss within epoch 105:  7.916141510009766\n",
      "Loss within epoch 110:  7.481141090393066\n",
      "Loss within epoch 115:  7.0904388427734375\n",
      "Loss within epoch 120:  6.737704277038574\n",
      "Loss within epoch 125:  6.417740821838379\n",
      "Loss within epoch 130:  6.126246452331543\n",
      "Loss within epoch 135:  5.8596343994140625\n",
      "Loss within epoch 140:  5.614891529083252\n",
      "Loss within epoch 145:  5.389458656311035\n",
      "Loss within epoch 150:  5.181169509887695\n",
      "Loss within epoch 155:  4.9881591796875\n",
      "Loss within epoch 160:  4.8088226318359375\n",
      "Loss within epoch 165:  4.641769886016846\n",
      "Loss within epoch 170:  4.485795021057129\n",
      "Loss within epoch 175:  4.339841842651367\n",
      "Loss within epoch 180:  4.202979564666748\n",
      "Loss within epoch 185:  4.074393272399902\n",
      "Loss within epoch 190:  3.953362226486206\n",
      "Loss within epoch 195:  3.8392419815063477\n",
      "Loss within epoch 200:  3.7314646244049072\n",
      "Loss within epoch 205:  3.629514455795288\n",
      "Loss within epoch 210:  3.5329408645629883\n",
      "Loss within epoch 215:  3.441328763961792\n",
      "Loss within epoch 220:  3.3543074131011963\n",
      "Loss within epoch 225:  3.271543502807617\n",
      "Loss within epoch 230:  3.192734956741333\n",
      "Loss within epoch 235:  3.117602825164795\n",
      "Loss within epoch 240:  3.045903444290161\n",
      "Loss within epoch 245:  2.977400779724121\n",
      "Loss within epoch 250:  2.9118921756744385\n",
      "Loss within epoch 255:  2.8491857051849365\n",
      "Loss within epoch 260:  2.7891037464141846\n",
      "Loss within epoch 265:  2.731487512588501\n",
      "Loss within epoch 270:  2.6761884689331055\n",
      "Loss within epoch 275:  2.6230719089508057\n",
      "Loss within epoch 280:  2.5720112323760986\n",
      "Loss within epoch 285:  2.522887706756592\n",
      "Loss within epoch 290:  2.475595474243164\n",
      "Loss within epoch 295:  2.4300341606140137\n",
      "Loss within epoch 300:  2.386110544204712\n",
      "Loss within epoch 305:  2.3437395095825195\n",
      "Loss within epoch 310:  2.3028383255004883\n",
      "Loss within epoch 315:  2.2633321285247803\n",
      "Loss within epoch 320:  2.225153684616089\n",
      "Loss within epoch 325:  2.1882340908050537\n",
      "Loss within epoch 330:  2.152515411376953\n",
      "Loss within epoch 335:  2.1179373264312744\n",
      "Loss within epoch 340:  2.0844509601593018\n",
      "Loss within epoch 345:  2.0519988536834717\n",
      "Loss within epoch 350:  2.020540237426758\n",
      "Loss within epoch 355:  1.990025281906128\n",
      "Loss within epoch 360:  1.9604148864746094\n",
      "Loss within epoch 365:  1.9316692352294922\n",
      "Loss within epoch 370:  1.9037513732910156\n",
      "Loss within epoch 375:  1.8766257762908936\n",
      "Loss within epoch 380:  1.85025954246521\n",
      "Loss within epoch 385:  1.8246185779571533\n",
      "Loss within epoch 390:  1.7996793985366821\n",
      "Loss within epoch 395:  1.7754085063934326\n",
      "Loss within epoch 400:  1.751783013343811\n",
      "Loss within epoch 405:  1.728772759437561\n",
      "Loss within epoch 410:  1.7063608169555664\n",
      "Loss within epoch 415:  1.6845169067382812\n",
      "Loss within epoch 420:  1.6632245779037476\n",
      "Loss within epoch 425:  1.6424620151519775\n",
      "Loss within epoch 430:  1.6222089529037476\n",
      "Loss within epoch 435:  1.6024501323699951\n",
      "Loss within epoch 440:  1.5831645727157593\n",
      "Loss within epoch 445:  1.5643352270126343\n",
      "Loss within epoch 450:  1.5459450483322144\n",
      "Loss within epoch 455:  1.5279840230941772\n",
      "Loss within epoch 460:  1.5104336738586426\n",
      "Loss within epoch 465:  1.4932806491851807\n",
      "Loss within epoch 470:  1.476511001586914\n",
      "Loss within epoch 475:  1.460114598274231\n",
      "Loss within epoch 480:  1.444075107574463\n",
      "Loss within epoch 485:  1.4283809661865234\n",
      "Loss within epoch 490:  1.413027286529541\n",
      "Loss within epoch 495:  1.397997498512268\n",
      "Loss within epoch 500:  1.3832838535308838\n",
      "Loss within epoch 505:  1.3688749074935913\n",
      "Loss within epoch 510:  1.3547642230987549\n",
      "Loss within epoch 515:  1.340938687324524\n",
      "Loss within epoch 520:  1.327392816543579\n",
      "Loss within epoch 525:  1.3141148090362549\n",
      "Loss within epoch 530:  1.301101803779602\n",
      "Loss within epoch 535:  1.2883434295654297\n",
      "Loss within epoch 540:  1.2758290767669678\n",
      "Loss within epoch 545:  1.2635571956634521\n",
      "Loss within epoch 550:  1.2515177726745605\n",
      "Loss within epoch 555:  1.2397068738937378\n",
      "Loss within epoch 560:  1.228114128112793\n",
      "Loss within epoch 565:  1.21673583984375\n",
      "Loss within epoch 570:  1.2055675983428955\n",
      "Loss within epoch 575:  1.1945998668670654\n",
      "Loss within epoch 580:  1.183829665184021\n",
      "Loss within epoch 585:  1.1732535362243652\n",
      "Loss within epoch 590:  1.1628608703613281\n",
      "Loss within epoch 595:  1.1526532173156738\n",
      "Loss within epoch 600:  1.142620325088501\n",
      "Loss within epoch 605:  1.1327619552612305\n",
      "Loss within epoch 610:  1.1230716705322266\n",
      "Loss within epoch 615:  1.1135444641113281\n",
      "Loss within epoch 620:  1.1041767597198486\n",
      "Loss within epoch 625:  1.094966173171997\n",
      "Loss within epoch 630:  1.0859073400497437\n",
      "Loss within epoch 635:  1.0769981145858765\n",
      "Loss within epoch 640:  1.0682318210601807\n",
      "Loss within epoch 645:  1.0596064329147339\n",
      "Loss within epoch 650:  1.0511188507080078\n",
      "Loss within epoch 655:  1.0427664518356323\n",
      "Loss within epoch 660:  1.0345466136932373\n",
      "Loss within epoch 665:  1.0264531373977661\n",
      "Loss within epoch 670:  1.018485426902771\n",
      "Loss within epoch 675:  1.010641098022461\n",
      "Loss within epoch 680:  1.0029159784317017\n",
      "Loss within epoch 685:  0.9953063726425171\n",
      "Loss within epoch 690:  0.987812340259552\n",
      "Loss within epoch 695:  0.9804297089576721\n",
      "Loss within epoch 700:  0.973159909248352\n",
      "Loss within epoch 705:  0.9659904837608337\n",
      "Loss within epoch 710:  0.9589313864707947\n",
      "Loss within epoch 715:  0.9519711136817932\n",
      "Loss within epoch 720:  0.9451116323471069\n",
      "Loss within epoch 725:  0.9383496046066284\n",
      "Loss within epoch 730:  0.9316830635070801\n",
      "Loss within epoch 735:  0.9251121282577515\n",
      "Loss within epoch 740:  0.9186317920684814\n",
      "Loss within epoch 745:  0.9122433066368103\n",
      "Loss within epoch 750:  0.9059393405914307\n",
      "Loss within epoch 755:  0.899724543094635\n",
      "Loss within epoch 760:  0.8935933709144592\n",
      "Loss within epoch 765:  0.8875442147254944\n",
      "Loss within epoch 770:  0.8815784454345703\n",
      "Loss within epoch 775:  0.8756899833679199\n",
      "Loss within epoch 780:  0.8698799014091492\n",
      "Loss within epoch 785:  0.8641458749771118\n",
      "Loss within epoch 790:  0.8584882616996765\n",
      "Loss within epoch 795:  0.8529027104377747\n",
      "Loss within epoch 800:  0.8473901748657227\n",
      "Loss within epoch 805:  0.8419491648674011\n",
      "Loss within epoch 810:  0.8365750312805176\n",
      "Loss within epoch 815:  0.8312684297561646\n",
      "Loss within epoch 820:  0.8260303139686584\n",
      "Loss within epoch 825:  0.8208589553833008\n",
      "Loss within epoch 830:  0.8157495856285095\n",
      "Loss within epoch 835:  0.8107037544250488\n",
      "Loss within epoch 840:  0.805720865726471\n",
      "Loss within epoch 845:  0.8007968664169312\n",
      "Loss within epoch 850:  0.7959349155426025\n",
      "Loss within epoch 855:  0.7911275029182434\n",
      "Loss within epoch 860:  0.7863817811012268\n",
      "Loss within epoch 865:  0.7816899418830872\n",
      "Loss within epoch 870:  0.7770567536354065\n",
      "Loss within epoch 875:  0.7724750638008118\n",
      "Loss within epoch 880:  0.7679486274719238\n",
      "Loss within epoch 885:  0.7634733319282532\n",
      "Loss within epoch 890:  0.7590500116348267\n",
      "Loss within epoch 895:  0.7546788454055786\n",
      "Loss within epoch 900:  0.750358521938324\n",
      "Loss within epoch 905:  0.7460829615592957\n",
      "Loss within epoch 910:  0.741859495639801\n",
      "Loss within epoch 915:  0.7376829385757446\n",
      "Loss within epoch 920:  0.7335495948791504\n",
      "Loss within epoch 925:  0.7294670939445496\n",
      "Loss within epoch 930:  0.7254255414009094\n",
      "Loss within epoch 935:  0.7214317321777344\n",
      "Loss within epoch 940:  0.7174818515777588\n",
      "Loss within epoch 945:  0.7135722637176514\n",
      "Loss within epoch 950:  0.7097072005271912\n",
      "Loss within epoch 955:  0.7058818340301514\n",
      "Loss within epoch 960:  0.7020972967147827\n",
      "Loss within epoch 965:  0.6983547210693359\n",
      "Loss within epoch 970:  0.694649338722229\n",
      "Loss within epoch 975:  0.6909850239753723\n",
      "Loss within epoch 980:  0.6873605847358704\n",
      "Loss within epoch 985:  0.683769941329956\n",
      "Loss within epoch 990:  0.6802184581756592\n",
      "Loss within epoch 995:  0.6767029166221619\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "model = CBOW(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE*2, embed_weights=None)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "# Training\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for ctx, tgt in data:\n",
    "        #### START YOUR CODE ####\n",
    "        context = torch.tensor(ctx, dtype=torch.long)\n",
    "        target = torch.tensor([tgt], dtype=torch.long)\n",
    "        pred = model(context)\n",
    "        loss = loss_function(pred, target)\n",
    "        total_loss += loss\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "    #optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print training information\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        print(f'Loss within epoch {epoch}: ', total_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "\n",
    "You should observe the loss decreasing from ~180 (at epoch 5) to ~0.7 (at epoch 995).\n",
    "The absolute values do not matter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "**1 point**\n",
    "\n",
    "In this final task, you will need to find the maximum index among the model output. *Hint*: use `torch.argmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_word(model_output, idx_to_word):\n",
    "    #### START YOUR CODE ####\n",
    "    word = idx_to_word[torch.argmax(model_output)]\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word is: \"abstract\"\n"
     ]
    }
   ],
   "source": [
    "# Test Task. Do not change the code blow\n",
    "ctx_words = 'processes manipulate other things called data.'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model(ctx_tensor)\n",
    "pred = get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "The predicted word is: \"abstract\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word is: \"spirits\"\n"
     ]
    }
   ],
   "source": [
    "# Test Task. Do not change the code blow\n",
    "ctx_words = 'we conjure the of the computer'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model(ctx_tensor)\n",
    "pred = get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "The predicted word is: \"spirits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2\n",
    "\n",
    "Here we will repeat the same exercise as Section 1 but start with pre-trained GloVe embeddings in included text file 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embedding_file):\n",
    "    g_embeddings = {}\n",
    "    g_vocab = []\n",
    "    with open(embedding_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            g_vocab.append(word)\n",
    "            vector = torch.tensor([float(val) for val in values[1:]])\n",
    "            g_embeddings[word] = vector\n",
    "    return g_embeddings, g_vocab\n",
    "\n",
    "# Load pretrained GloVe embeddings and extract vocabulary\n",
    "glove_file = 'glove.6B.50d.txt'\n",
    "g_embeddings, g_vocab = load_glove_embeddings(glove_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test to Make Sure GloVe Embeddings are Loaded Correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_vocab_size 400001\n",
      "g_embedding_matrix.shape torch.Size([400001, 50])\n",
      "g_word_index 0\n"
     ]
    }
   ],
   "source": [
    "print ('g_vocab_size', len(g_vocab))\n",
    "\n",
    "g_word_to_idx = {word: i for i, word in enumerate(g_vocab)}\n",
    "g_idx_to_word = list(g_vocab)\n",
    "\n",
    "g_vocab_size, g_embedding_dim = len (g_vocab),50\n",
    "\n",
    "g_embedding_matrix = torch.stack(list(g_embeddings.values()))\n",
    "\n",
    "print ('g_embedding_matrix.shape',g_embedding_matrix.shape)\n",
    "# Create PyTorch embedding layer\n",
    "\n",
    "g_embedding_layer = nn.Embedding.from_pretrained(g_embedding_matrix)\n",
    "word = 'the'\n",
    "g_word_index = g_vocab.index(word)\n",
    "\n",
    "print ('g_word_index', g_word_index)\n",
    "# Access pretrained embeddings for specific words\n",
    "#g_word_embedding = g_embedding_layer(torch.tensor([g_word_index]))\n",
    "\n",
    "#print('g_word_embedding',g_word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "g_vocab_size 400001<br>\n",
    "g_embedding_matrix.shape torch.Size([400001, 50])<br>\n",
    "g_word_index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n",
    "\n",
    "**2 points**\n",
    "\n",
    "Cosine similarity metric ranges from -1 to 1. Larger values indicate more similarity.\n",
    "\n",
    "hint: use F.cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine sim1: 0.8609580993652344\n",
      "Cosine sim2: 0.7839043736457825\n"
     ]
    }
   ],
   "source": [
    "word_king = 'king'\n",
    "word_man ='man'\n",
    "word_woman = 'woman'\n",
    "word_queen = 'queen'\n",
    "\n",
    "\n",
    "king_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_king)]))\n",
    "man_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_man)]))\n",
    "woman_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_woman)]))\n",
    "queen_embedding = g_embedding_layer(torch.tensor([g_vocab.index(word_queen)]))\n",
    "\n",
    "word_embedding_case_1 = king_embedding - man_embedding + woman_embedding\n",
    "\n",
    "##Start your Code\n",
    "#Obtain the cosine similarity between king-man+woman to queen in \"cosine_sim1\"\n",
    "cosine_sim1 = F.cosine_similarity(word_embedding_case_1, queen_embedding)\n",
    "cosine_sim2 = F.cosine_similarity(king_embedding, queen_embedding)\n",
    "#Obtain the cosine similarity between king and queen in \"cosine_sim2\"\n",
    "\n",
    "\n",
    "#End your code\n",
    "print(\"Cosine sim1:\", cosine_sim1.item())\n",
    "print(\"Cosine sim2:\", cosine_sim2.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "Cosine sim1: 0.8609580993652344<br>\n",
    "Cosine sim2: 0.7839043140411377\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g torch.Size([400001, 50])\n",
      "n torch.Size([49, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryce\\AppData\\Local\\Temp\\ipykernel_29756\\452055175.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  n_embedding_matrix[i] = torch.tensor(g_embedding_matrix[index], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "#First we have to convert indexes from our toy vocabulary to those used in GloVe Vocabulary\n",
    "#and create a new embedding_matrix that contains embeddings only for words in the smaller \n",
    "#vocabulary\n",
    "import re\n",
    "\n",
    "#print ('len(vocab)', len(vocab))\n",
    "def preprocess_word(word):\n",
    "    # Convert word to lowercase\n",
    "    word = word.lower()\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word\n",
    "\n",
    "n_embedding_matrix = torch.zeros(len(vocab), g_embedding_matrix.shape[1])\n",
    "#n_embedding_layer = nn.Embedding.from_pretrained(n_embedding_matrix)\n",
    "print ('g', g_embedding_matrix.shape)\n",
    "print ('n', n_embedding_matrix.shape)\n",
    "\n",
    "new_vocab_indices = {}\n",
    "for i, word in enumerate (vocab):\n",
    "    word=preprocess_word(word)\n",
    "    if word in g_vocab:\n",
    "        #new_vocab_indices[word] = g_vocab.index(word)\n",
    "        index = g_vocab.index(word)\n",
    "        n_embedding_matrix[i] = torch.tensor(g_embedding_matrix[index], dtype=torch.float)\n",
    "    else:\n",
    "        print ('This word not in GloVe',word)\n",
    "#n_embedding_layer = nn.Embedding.from_pretrained(n_embedding_matrix)    \n",
    "#new_word_indices = [new_vocab_indices[preprocess_word(w)] for w in raw_text]        \n",
    "        \n",
    "#print ('new_vocab_indices',new_vocab_indices)\n",
    "#print ('new_word_indices',new_word_indices)\n",
    "#print ('len(new_word_indices)',len(new_word_indices)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "g torch.Size([400001, 50])<br>\n",
    "n torch.Size([49, 50])<br>\n",
    "\n",
    "Ignore any warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop Starting with Pre-Trained Embeddings from GloVe\n",
    "\n",
    "**3 points**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss within epoch 5:  201.59201049804688\n",
      "Loss within epoch 10:  178.2009735107422\n",
      "Loss within epoch 15:  161.1248779296875\n",
      "Loss within epoch 20:  146.86416625976562\n",
      "Loss within epoch 25:  134.7068328857422\n",
      "Loss within epoch 30:  124.19705963134766\n",
      "Loss within epoch 35:  115.00506591796875\n",
      "Loss within epoch 40:  106.88453674316406\n",
      "Loss within epoch 45:  99.64949798583984\n",
      "Loss within epoch 50:  93.15776062011719\n",
      "Loss within epoch 55:  87.29912567138672\n",
      "Loss within epoch 60:  81.98626708984375\n",
      "Loss within epoch 65:  77.14881134033203\n",
      "Loss within epoch 70:  72.72893524169922\n",
      "Loss within epoch 75:  68.67851257324219\n",
      "Loss within epoch 80:  64.95677947998047\n",
      "Loss within epoch 85:  61.528926849365234\n",
      "Loss within epoch 90:  58.364959716796875\n",
      "Loss within epoch 95:  55.43872833251953\n",
      "Loss within epoch 100:  52.727420806884766\n",
      "Loss within epoch 105:  50.21091842651367\n",
      "Loss within epoch 110:  47.87141036987305\n",
      "Loss within epoch 115:  45.69314956665039\n",
      "Loss within epoch 120:  43.662010192871094\n",
      "Loss within epoch 125:  41.76542663574219\n",
      "Loss within epoch 130:  39.992122650146484\n",
      "Loss within epoch 135:  38.33193588256836\n",
      "Loss within epoch 140:  36.77570724487305\n",
      "Loss within epoch 145:  35.31520080566406\n",
      "Loss within epoch 150:  33.94292449951172\n",
      "Loss within epoch 155:  32.65209197998047\n",
      "Loss within epoch 160:  31.43654441833496\n",
      "Loss within epoch 165:  30.29068374633789\n",
      "Loss within epoch 170:  29.209386825561523\n",
      "Loss within epoch 175:  28.188003540039062\n",
      "Loss within epoch 180:  27.222270965576172\n",
      "Loss within epoch 185:  26.308269500732422\n",
      "Loss within epoch 190:  25.44244956970215\n",
      "Loss within epoch 195:  24.621505737304688\n",
      "Loss within epoch 200:  23.84244155883789\n",
      "Loss within epoch 205:  23.102474212646484\n",
      "Loss within epoch 210:  22.399059295654297\n",
      "Loss within epoch 215:  21.729833602905273\n",
      "Loss within epoch 220:  21.092632293701172\n",
      "Loss within epoch 225:  20.485448837280273\n",
      "Loss within epoch 230:  19.906423568725586\n",
      "Loss within epoch 235:  19.353851318359375\n",
      "Loss within epoch 240:  18.826129913330078\n",
      "Loss within epoch 245:  18.32179069519043\n",
      "Loss within epoch 250:  17.839462280273438\n",
      "Loss within epoch 255:  17.377870559692383\n",
      "Loss within epoch 260:  16.935842514038086\n",
      "Loss within epoch 265:  16.51226234436035\n",
      "Loss within epoch 270:  16.106111526489258\n",
      "Loss within epoch 275:  15.716444969177246\n",
      "Loss within epoch 280:  15.342351913452148\n",
      "Loss within epoch 285:  14.983009338378906\n",
      "Loss within epoch 290:  14.637633323669434\n",
      "Loss within epoch 295:  14.305505752563477\n",
      "Loss within epoch 300:  13.985929489135742\n",
      "Loss within epoch 305:  13.678279876708984\n",
      "Loss within epoch 310:  13.381950378417969\n",
      "Loss within epoch 315:  13.09638500213623\n",
      "Loss within epoch 320:  12.82104778289795\n",
      "Loss within epoch 325:  12.555451393127441\n",
      "Loss within epoch 330:  12.29911994934082\n",
      "Loss within epoch 335:  12.05162525177002\n",
      "Loss within epoch 340:  11.81255054473877\n",
      "Loss within epoch 345:  11.581502914428711\n",
      "Loss within epoch 350:  11.358122825622559\n",
      "Loss within epoch 355:  11.142057418823242\n",
      "Loss within epoch 360:  10.9329833984375\n",
      "Loss within epoch 365:  10.730594635009766\n",
      "Loss within epoch 370:  10.534591674804688\n",
      "Loss within epoch 375:  10.344708442687988\n",
      "Loss within epoch 380:  10.160674095153809\n",
      "Loss within epoch 385:  9.982251167297363\n",
      "Loss within epoch 390:  9.809198379516602\n",
      "Loss within epoch 395:  9.6412992477417\n",
      "Loss within epoch 400:  9.478336334228516\n",
      "Loss within epoch 405:  9.320120811462402\n",
      "Loss within epoch 410:  9.166450500488281\n",
      "Loss within epoch 415:  9.0171537399292\n",
      "Loss within epoch 420:  8.872056007385254\n",
      "Loss within epoch 425:  8.730996131896973\n",
      "Loss within epoch 430:  8.593819618225098\n",
      "Loss within epoch 435:  8.460372924804688\n",
      "Loss within epoch 440:  8.330524444580078\n",
      "Loss within epoch 445:  8.204131126403809\n",
      "Loss within epoch 450:  8.081071853637695\n",
      "Loss within epoch 455:  7.961225986480713\n",
      "Loss within epoch 460:  7.844475746154785\n",
      "Loss within epoch 465:  7.7307047843933105\n",
      "Loss within epoch 470:  7.619816780090332\n",
      "Loss within epoch 475:  7.511704444885254\n",
      "Loss within epoch 480:  7.406271457672119\n",
      "Loss within epoch 485:  7.303427696228027\n",
      "Loss within epoch 490:  7.203085422515869\n",
      "Loss within epoch 495:  7.105154037475586\n",
      "Loss within epoch 500:  7.009561061859131\n",
      "Loss within epoch 505:  6.916223526000977\n",
      "Loss within epoch 510:  6.825068473815918\n",
      "Loss within epoch 515:  6.736025810241699\n",
      "Loss within epoch 520:  6.649024963378906\n",
      "Loss within epoch 525:  6.5640034675598145\n",
      "Loss within epoch 530:  6.480896472930908\n",
      "Loss within epoch 535:  6.399641990661621\n",
      "Loss within epoch 540:  6.320187568664551\n",
      "Loss within epoch 545:  6.242473125457764\n",
      "Loss within epoch 550:  6.166445255279541\n",
      "Loss within epoch 555:  6.092056751251221\n",
      "Loss within epoch 560:  6.019254684448242\n",
      "Loss within epoch 565:  5.94799280166626\n",
      "Loss within epoch 570:  5.87822151184082\n",
      "Loss within epoch 575:  5.8099045753479\n",
      "Loss within epoch 580:  5.742992877960205\n",
      "Loss within epoch 585:  5.6774492263793945\n",
      "Loss within epoch 590:  5.613231182098389\n",
      "Loss within epoch 595:  5.550307750701904\n",
      "Loss within epoch 600:  5.488635063171387\n",
      "Loss within epoch 605:  5.428182601928711\n",
      "Loss within epoch 610:  5.368912696838379\n",
      "Loss within epoch 615:  5.310795307159424\n",
      "Loss within epoch 620:  5.25379753112793\n",
      "Loss within epoch 625:  5.197891712188721\n",
      "Loss within epoch 630:  5.143046855926514\n",
      "Loss within epoch 635:  5.0892333984375\n",
      "Loss within epoch 640:  5.036425590515137\n",
      "Loss within epoch 645:  4.984596252441406\n",
      "Loss within epoch 650:  4.933719158172607\n",
      "Loss within epoch 655:  4.883770942687988\n",
      "Loss within epoch 660:  4.834728240966797\n",
      "Loss within epoch 665:  4.786563396453857\n",
      "Loss within epoch 670:  4.739258289337158\n",
      "Loss within epoch 675:  4.692790985107422\n",
      "Loss within epoch 680:  4.647143363952637\n",
      "Loss within epoch 685:  4.6022868156433105\n",
      "Loss within epoch 690:  4.558206558227539\n",
      "Loss within epoch 695:  4.514885902404785\n",
      "Loss within epoch 700:  4.47230339050293\n",
      "Loss within epoch 705:  4.430441856384277\n",
      "Loss within epoch 710:  4.389283657073975\n",
      "Loss within epoch 715:  4.348813533782959\n",
      "Loss within epoch 720:  4.309016227722168\n",
      "Loss within epoch 725:  4.269871711730957\n",
      "Loss within epoch 730:  4.23136568069458\n",
      "Loss within epoch 735:  4.193485260009766\n",
      "Loss within epoch 740:  4.156216621398926\n",
      "Loss within epoch 745:  4.119543552398682\n",
      "Loss within epoch 750:  4.08345365524292\n",
      "Loss within epoch 755:  4.0479350090026855\n",
      "Loss within epoch 760:  4.012971878051758\n",
      "Loss within epoch 765:  3.9785537719726562\n",
      "Loss within epoch 770:  3.9446680545806885\n",
      "Loss within epoch 775:  3.911302328109741\n",
      "Loss within epoch 780:  3.8784470558166504\n",
      "Loss within epoch 785:  3.8460917472839355\n",
      "Loss within epoch 790:  3.8142223358154297\n",
      "Loss within epoch 795:  3.7828292846679688\n",
      "Loss within epoch 800:  3.7519052028656006\n",
      "Loss within epoch 805:  3.7214372158050537\n",
      "Loss within epoch 810:  3.6914186477661133\n",
      "Loss within epoch 815:  3.6618361473083496\n",
      "Loss within epoch 820:  3.632683277130127\n",
      "Loss within epoch 825:  3.603952169418335\n",
      "Loss within epoch 830:  3.5756309032440186\n",
      "Loss within epoch 835:  3.5477139949798584\n",
      "Loss within epoch 840:  3.5201919078826904\n",
      "Loss within epoch 845:  3.493055820465088\n",
      "Loss within epoch 850:  3.4663007259368896\n",
      "Loss within epoch 855:  3.4399168491363525\n",
      "Loss within epoch 860:  3.4138967990875244\n",
      "Loss within epoch 865:  3.3882334232330322\n",
      "Loss within epoch 870:  3.362921953201294\n",
      "Loss within epoch 875:  3.337951898574829\n",
      "Loss within epoch 880:  3.313322067260742\n",
      "Loss within epoch 885:  3.289018392562866\n",
      "Loss within epoch 890:  3.265040636062622\n",
      "Loss within epoch 895:  3.2413806915283203\n",
      "Loss within epoch 900:  3.2180309295654297\n",
      "Loss within epoch 905:  3.1949899196624756\n",
      "Loss within epoch 910:  3.1722466945648193\n",
      "Loss within epoch 915:  3.1497981548309326\n",
      "Loss within epoch 920:  3.1276395320892334\n",
      "Loss within epoch 925:  3.1057655811309814\n",
      "Loss within epoch 930:  3.084169864654541\n",
      "Loss within epoch 935:  3.06284499168396\n",
      "Loss within epoch 940:  3.0417919158935547\n",
      "Loss within epoch 945:  3.0210015773773193\n",
      "Loss within epoch 950:  3.000472068786621\n",
      "Loss within epoch 955:  2.9801950454711914\n",
      "Loss within epoch 960:  2.960167407989502\n",
      "Loss within epoch 965:  2.940385341644287\n",
      "Loss within epoch 970:  2.920844793319702\n",
      "Loss within epoch 975:  2.9015395641326904\n",
      "Loss within epoch 980:  2.8824691772460938\n",
      "Loss within epoch 985:  2.8636274337768555\n",
      "Loss within epoch 990:  2.8450112342834473\n",
      "Loss within epoch 995:  2.826615333557129\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "EMDEDDING_DIM = 50\n",
    "#print ('vocab_size',vocab_size)\n",
    "#print ('NE Matric.shape', n_embedding_matrix.shape)\n",
    "model2 = CBOW(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE*2, n_embedding_matrix)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=0.001)\n",
    "optimizer.zero_grad()\n",
    "# Training\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "\n",
    "    for ctx, tgt in data:\n",
    "        #### START YOUR CODE ####\n",
    "        context = torch.tensor(ctx, dtype=torch.long)\n",
    "        target = torch.tensor([tgt], dtype=torch.long)\n",
    "        pred = model2(context)\n",
    "        loss = loss_function(pred, target)\n",
    "        total_loss += loss\n",
    "        #### END YOUR CODE ####\n",
    "\n",
    "    #optimize at the end of each epoch\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print training information\n",
    "    if epoch % 5 == 0 and epoch > 0:\n",
    "        print(f'Loss within epoch {epoch}: ', total_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "You should observe the loss decreasing from ~201 (at epoch 5) to ~2.8 (at epoch 995).\n",
    "The absolute values do not matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_get_predicted_word(output, idx_to_word):\n",
    "    #### START YOUR CODE ####\n",
    "    #Use the code from Task 4 above\n",
    "    word = idx_to_word[torch.argmax(output)]\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word is: \"spirits\"\n"
     ]
    }
   ],
   "source": [
    "# Test Task. Do not change the code blow\n",
    "ctx_words = 'we conjure the of the computer'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model2(ctx_tensor)\n",
    "pred = new_get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "The predicted word is: \"spirits\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word is: \"abstract\"\n"
     ]
    }
   ],
   "source": [
    "# Test Task. Do not change the code blow\n",
    "ctx_words = 'processes manipulate other things called data.'.split()\n",
    "ctx_indices = [word_to_idx[w] for w in ctx_words]\n",
    "ctx_tensor = torch.tensor(ctx_indices, dtype=torch.long)\n",
    "\n",
    "out = model2(ctx_tensor)\n",
    "pred = new_get_predicted_word(out, idx_to_word)\n",
    "print(f'The predicted word is: \\\"{pred}\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "The predicted word is: \"abstract\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "Since our example text paragraph is rather small with a small vocabulary, the real utility of using pre-trained embeddings is not evident. It would be evident if we had a rather large text file. The purpose of section 2 is to familarize you with how to use pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
